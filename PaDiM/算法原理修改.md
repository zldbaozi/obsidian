## 1、在修改的时候，出现了训练和推理的tensor不匹配的问题

PaDiM算法的核心是将三层特征拼接在一起，为了能拼接，这三个不同尺寸的特征图必须变成相同的尺寸——上采样或者下采样

1. **Layer 1 (Stride 4):** 112/4=28×28
    
2. **Layer 2 (Stride 8):** 112/8=14×14
    
3. **Layer 3 (Stride 16):** 112/16=7×7
    
![[Pasted image 20251230111829.png|1125]]

上采样保证高精度，下采样保证高速度，这里选择的是上采样。
```c++
def _extract_multiscale_features(self, x):
    features = self.feature_extractor(x)
    
    import torch.nn.functional as F
    # 假设 layer1 是最大的 (28x28)
    
    target_size = features[0].shape[-2:]  
    # 表示第一层特征图的高度和宽度，shape[-2:]是python的切片操作，表示从倒数第2个维度开始，取到最后一个维度[B,C,H,W]
    
    resized = [F.interpolate(f, size=target_size, mode='bilinear', align_corners=False) for f in features]
    # 将不同层的特征图调整到相同的尺寸，以便后续拼接。f表示列表中的每一层特征图，F.interpolate()将特征图调整到target_size,bilinear表示双线性插值,align_corners表示避免插值时的边缘对齐问题
    
    out = torch.cat(resized, dim=1)
    # 将所有特征图在通道维度为1上拼接
    
    return out.permute(0, 2, 3, 1) # [B, H, W, C]
```

# 2、python转c++优化推理时间
## 1.随机降维投影优化
原来是scikit-learn（cpu）版本的，速度慢，改为pytorch随机索引切片（矩阵乘法直接随机选100个通道，其余直接舍弃，减少计算）
```c++
# 第二步：随机投影降维（cpu版）

        if reduce_dims and reduce_dims < all_features.shape[-1]:

            print(f"使用随机投影降维: {all_features.shape[-1]} -> {reduce_dims}")

            self.projector = SparseRandomProjection(n_components=reduce_dims)

            # 重塑特征以便投影

            original_shape = all_features.shape

            print("重塑特征矩阵以进行随机投影...")

            all_features_flat = all_features.reshape(-1, original_shape[-1])

            # 执行随机投影并显示进度

            print("正在进行随机投影...")

            all_features_projected = self.projector.fit_transform(all_features_flat)

            print("✅ 随机投影完成!")
```

```c++
# 随机降维(优化后)
        if reduce_dims < C:
            self.selected_indices = torch.randperm(C)[:reduce_dims].to(self.device)
            
            embedding_vectors = torch.index_select(torch.from_numpy(embedding_vectors).to(self.device), 3 ,self.selected_indices).cpu().numpy()
            # torch.from_numpy(embedding_vectors)将原始特征向量(embedding_vectors,numpy)转换为pytorch张量
            # .to(self.device)将张量移动到指定设备
            # torch.index_select()选择特定维度上的子集
            C = reduce_dims
```

## 2.马氏距离计算优化
```c++
# 为每个位置计算马氏距离 ---逐像素循环
        for i in range(H):
            for j in range(W):
                # 获取测试特征
                test_features = features_np[:, i, j, :]  # [B, C]
                
                mean = self.means[i, j]  # [C]
                
                inv_cov = self.inv_covs[i, j]  # [C, C]
                
                diff = test_features - mean  # [B, C]
               
                temp = np.dot(diff, inv_cov)  # [B, C]

                mahalanobis_dist = np.sum(temp * diff, axis=1)  # [B]
                mahalanobis_dist = np.sqrt(np.abs(mahalanobis_dist))

                anomaly_maps[:, i, j] = mahalanobis_dist
```
之前的逐像素计算使用numpy，所有操作都在CPU上完成(无法使用GPU加速，数据需要频繁再numpy和pytorch张量之间转换增加开销)
```c++
# 广播机制计算 (x - μ)
diff = features - self.means.unsqueeze(0)  # [B, H, W, C]

# 使用 einsum 进行批量矩阵乘法
# torch.einsum是pytorch中的爱因斯坦求和约定函数，用于高效地执行张量操作，它的作用是根据指定的索引规则，对输入张量进行维度缩并或变换。
# b: batch, h: height, w: width, i: c_in, j: c_out
#diff的最后一个维度i和inv_covs的倒数第二个维度i是公共维度，这两个维度的元素会逐一相乘并求和，得到结果张量的维度bhwj
#为什么公共维度是i,而不是h,w。因为矩阵乘法的公共维度必须是特征维度，而不是空间维度h,w
temp = torch.einsum('bhwi,hwij->bhwj', diff, self.inv_covs)  # [B, H, W, C]

# 计算马氏距离
mahalanobis_dist = torch.einsum('bhwj,bhwj->bhw', temp, diff)  # [B, H, W]
mahalanobis_dist = torch.sqrt(torch.abs(mahalanobis_dist))
```
现在使用torch的广播机制和einsum一次性对整个特征图的所有像素位置进行计算，所有操作都在GPU上完成(数据始终保持在GPU上，无需频繁转换；einsum是高效的张量操作，能够充分利用GPU的并行计算能力)


==*公共维度的理解*==
*diff:[B,H,W,C],b表示批量大小，h表示特征图的高度，w表示特征图的宽度，i表示输入通道*
*inv_covs:[H,W,C,C],h表示特征图的高度，w表示特征图的宽度，i表示输入通道，j表示输出通道*

假设：
- `B = 2（批量大小为 2）
- `H = 3,W = 3（特征图大小为 3x3）
- `C = 4（特征通道数为 4）。
那么：
- [diff] 的形状是 `[2, 3, 3, 4]`，即每个像素位置 `(h, w)` 有一个长度为 4 的特征向量，也就是1x4的向量。
- [inv_covs] 的形状是 `[3, 3, 4, 4]`，即每个像素位置 `(h, w)` 有一个 4x4 的协方差逆矩阵。
所以公共维度就是`i = 4
``
``mahalanobis_dist = torch.einsum('bhwj,bhwj->bhw', temp, diff)  # [B, H, W]
这行代码的公共维度是j,也就是两个1x4的特征向量相乘，不满足矩阵乘法的要求（第一个矩阵的列数等于第二个矩阵的行数）

为了解决这种问题，可以使矩阵转置后相乘——————涉及到内积还是外积的问题（内积可转置，外积不可以）
==*内积（点积）：表示两个向量的对应元素逐一相乘并求和，结果是一个标量==*
==*外积：表示两个向量的所有元素两两相乘，结果是一个矩阵*==



